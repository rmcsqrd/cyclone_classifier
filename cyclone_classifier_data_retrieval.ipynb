{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219f571-d69c-40a7-9156-0ec4bb26d011",
   "metadata": {},
   "source": [
    "# Storm Data Retrieval/Association with Moisture Vapor\n",
    "This notebook focuses on the data collection/processing. It contains two main sources:\n",
    "\n",
    "- GOES-16 satellite data (https://docs.opendata.aws/noaa-goes16/cics-readme.html)\n",
    "- IBTRaCS tropical storm data (https://www.ncdc.noaa.gov/ibtracs/index.php?name=ib-v4-access)\n",
    "\n",
    "Together, these sources form a data set that will be used to train a model to identify tropical cyclones. The main workflow is as follows:\n",
    "\n",
    "1. Download full disk moisture vapor (mv) data (ABI channel 9) from GOES-16 via AWS\n",
    "2. Parse the IBTRaCS data for storms from 2016-now. Do this by creating a dictionary where the key is a datetime and the value is information about the storm\n",
    "3. Associate the storm data with the mv data to create a labeled data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cf2f6c-65b1-4f0c-9c1e-c88210e7af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "from datetime import datetime\n",
    "from tools.aws_goes import GOESArchiveDownloader, GOESProduct, save_s3_product\n",
    "import xarray as xr\n",
    "import metpy\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# jupyter notebook config\n",
    "# %matplotlib inline #suppress plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c703629-f06d-4350-a492-ac6cd5a59610",
   "metadata": {},
   "source": [
    "### Step 1: Get GOES-16 Data\n",
    "Start by downloading hosted GOES-16 data from AWS using a the custom functions found in `aws_goes.py`. The main idea of this is to collect GOES-16 data in 12 hour increments. GOES-16 beams down full-disk data every ~10 minutes which would be a lot of data. Twice a day seems like a more reasonable resolution for our dataset but this is totally subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919156a4-d3d4-4110-9546-d4514b35dc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [2:29:45<00:00,  6.15s/it] \n"
     ]
    }
   ],
   "source": [
    "# setup params\n",
    "# startdate = datetime(2017, 9, 16, 23, 59, 0) hurricane maria dates\n",
    "# enddate = datetime(2017, 10, 2, 23, 59, 0)\n",
    "startdate = datetime(2017, 1, 1, 23, 59, 0)\n",
    "enddate = datetime(2021, 1, 1, 23, 59, 0)\n",
    "outpath = \"/Users/rmcmahon/dev/cyclone_classifier/data/aws_download\"\n",
    "arc = GOESArchiveDownloader()\n",
    "ABI_prods = arc.get_range(startdate, enddate, GOESProduct(typ='ABI', channel=9, sector='full'), day_length='half')#, satellite = 'goes17'))\n",
    "\n",
    "# download data\n",
    "for s3obj in tqdm(ABI_prods):\n",
    "    save_s3_product(s3obj, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d3f2e-5d49-4d28-b2f9-7a07169da133",
   "metadata": {},
   "source": [
    "### Step 2: Get Storm Data\n",
    "Start by downloading the IBTRaCS data from the web (https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/netcdf/). In this case it is convenient to download the data since 1980 because our GOES-16 data starts in 2017. We then create a dictionary with `datetime` keys and storm information as the values. We simplify our data by only looking at tropical storms after ~12/2016 and storms with wind speeds > 34 knots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab5152c-af0a-4acb-b577-91ab5558b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load storm data\n",
    "stormfp = 'data/storm/IBTrACS.since1980.v04r00.nc'  # all storm data since 1980 (much smaller dataset)\n",
    "stormdata = xr.open_dataset(stormfp)\n",
    "\n",
    "# figure out bounds between storms\n",
    "first_storm_times = stormdata.isel(storm=[0]).time.values\n",
    "last_storm_times = stormdata.isel(storm=[len(stormdata.storm)-1]).time.values\n",
    "start_time = first_storm_times[~np.isnat(first_storm_times)].min()\n",
    "end_time = last_storm_times[~np.isnat(last_storm_times)].max()\n",
    "\n",
    "# generate date range\n",
    "time_range = pd.date_range(start=start_time, end=end_time, freq='3H').values\n",
    "time_range = time_range.astype('datetime64[s]')  # clip off the nanoseconds\n",
    "\n",
    "# create storm dictionary to store storms\n",
    "storm_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67521bf1-8800-42a0-9b70-83de49760d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484/484 [00:34<00:00, 14.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# define some functions to help with time conversion\n",
    "\n",
    "'''\n",
    "Determine storm classification based on Saffir Simpson Scale\n",
    "    Although there are several reporting agencies, we use US reporting because our GOES data is focused on full disk encompassing US \n",
    "'''\n",
    "goes16_begin = 4000  # storm #4000 in ibtracs is around Dec 2016, we start here to reduce amount of storms we need to search through to generate storm_dict\n",
    "for storm_id in tqdm(stormdata.storm[4000:].values):\n",
    "    storm = stormdata.isel(storm=storm_id)\n",
    "    storm_times = storm.time.values[~np.isnat(storm.time.values)]\n",
    "    wind_vals = storm.usa_wind.values\n",
    "    for time_idx, time in enumerate(storm_times):\n",
    "        storm_time = pd.Timestamp(time).round('10min').to_pydatetime()\n",
    "        latlonname = storm.time[time_idx].lat.values, storm.time[time_idx].lon.values, storm.name.values\n",
    "        if wind_vals[time_idx] >= 34:  # >34 knots => tropical storm via Saffir-Simpson scale\n",
    "            if storm_time in storm_dict:\n",
    "                # key is rounded time, value is tuple (or maybe a class for readability and other features)\n",
    "                # need a conditional check so we don't write in the same rounded value\n",
    "                if (storm_time, latlonname) not in storm_dict.items():\n",
    "                    storm_dict[storm_time].append(latlonname)\n",
    "            else:\n",
    "                storm_dict[storm_time] = [latlonname]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6cb79-4e71-472f-b363-265e54499124",
   "metadata": {},
   "source": [
    "### Step 3: Label GOES-16 Data with Storm Data\n",
    "Now that we've downloaded the data and created a dictionary of storms by time stamp, we finally create the data set. This dataset contains three types of output per GOES-16 image:\n",
    "\n",
    "1. Unlabeled moisture vapor image\n",
    "2. Labeled moisture vapor image - this has a bounding box drawn around a tropical storm (if one exists)\n",
    "3. Labeled moisture vapor image with embedded data - same as the labeled mv image but also labeled with storm name/time information. \n",
    "\n",
    "Items 1,2 are used for training and validation of the model. Item 3 is used for model verification and training data verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c20433-c552-4722-9c3f-14c96fa7f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 20/1460 [01:55<2:14:16,  5.59s/it]<ipython-input-5-27b2a5c6844e>:39: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(figsize=(15, 12))\n",
      " 74%|███████▍  | 1083/1460 [1:42:18<37:27,  5.96s/it] /Users/rmcmahon/opt/anaconda3/envs/cyclone_classifier_env/lib/python3.9/site-packages/matplotlib/image.py:446: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
      "/Users/rmcmahon/opt/anaconda3/envs/cyclone_classifier_env/lib/python3.9/site-packages/matplotlib/image.py:453: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "/Users/rmcmahon/opt/anaconda3/envs/cyclone_classifier_env/lib/python3.9/site-packages/matplotlib/image.py:458: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n",
      "100%|██████████| 1460/1460 [2:20:08<00:00,  5.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# define function to unpack storm data\n",
    "def unpack_centers_names(data, patch_size):\n",
    "    # use this function to unpack the tuples\n",
    "    # tuples are stored like (lat, lon, name)\n",
    "    lt = lambda l, t: [i[t] for i in l]\n",
    "    lats = lt(data, 0)\n",
    "    lons = lt(data, 1)\n",
    "    names = lt(data, 2)\n",
    "    \n",
    "    patch_lons = [i-patch_size/2 for i in lons]\n",
    "    patch_lats = [i-patch_size/2 for i in lats]\n",
    "    patch_centers = list(zip(patch_lons, patch_lats))\n",
    "    return patch_centers, names\n",
    "\n",
    "# navigate and process each file that was downloaded by creating and saving matplotlib image\n",
    "nc_files = [os.path.join(outpath,file) for file in os.listdir(outpath) if os.path.splitext(file)[1] == '.nc']\n",
    "\n",
    "# define save paths and create if they don't exist\n",
    "raw_path = 'data/dataset/raw'\n",
    "labeled_path = 'data/dataset/labeled'\n",
    "meta_path = 'data/dataset/meta'\n",
    "\n",
    "for path in [raw_path, labeled_path, meta_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# loop through files and create data set\n",
    "for fp in tqdm(nc_files):\n",
    "    # start by unpacking data and getting timestamp of GOES data\n",
    "    ds = xr.open_dataset(fp)\n",
    "    mv_time = ds.t.values\n",
    "    mv_time_rounded = pd.Timestamp(mv_time).round('3H').to_pydatetime()\n",
    "    timestamp_string = mv_time_rounded.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    \n",
    "    # next, draw goes mv image and save\n",
    "    dat = ds.metpy.parse_cf('Rad')\n",
    "    geos = dat.metpy.cartopy_crs\n",
    "    x,y = dat.x, dat.y\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    ax = plt.axes(projection = geos)\n",
    "    ds_data = ds['Rad'].data\n",
    "    ax.imshow(ds_data, origin='upper', extent=(x.min(), x.max(), y.min(), y.max()), transform=geos);\n",
    "    plt.savefig(os.path.join(raw_path, timestamp_string+\"_raw.png\"))\n",
    "    \n",
    "    # next draw on the storms if they exist by checking time of storm against storm_dict and save\n",
    "    patch_size = 5\n",
    "    storm_names = []\n",
    "    if mv_time_rounded in storm_dict:\n",
    "        patch_centers, storm_names = unpack_centers_names(storm_dict[mv_time_rounded], patch_size)\n",
    "        for center in patch_centers:\n",
    "            rect = patches.Rectangle(center, patch_size, patch_size, linewidth=1, edgecolor='r', facecolor='r', alpha=0.5, transform=ccrs.Geodetic())\n",
    "            ax.add_patch(rect);\n",
    "    plt.savefig(os.path.join(labeled_path, timestamp_string+\"_labeled.png\"));\n",
    "    \n",
    "    # finally add meta data\n",
    "    plt.title(\"DATETIME:{}\\nTROPICAL STORMS:{}\".format(timestamp_string, [name.tolist().decode() for name in storm_names]));\n",
    "    ax.coastlines(resolution='50m', color='black', linewidth=0.25);\n",
    "    ax.add_feature(ccrs.cartopy.feature.STATES, linewidth=0.25);\n",
    "    plt.savefig(os.path.join(meta_path, timestamp_string+\"_meta.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
